{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install py7zr jiwer\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport torch\nimport torch.nn as nn\nfrom torch import optim\nfrom torch.utils.data import DataLoader, Dataset\nimport torchaudio\nimport py7zr\nfrom glob import glob\nimport librosa as ls\nimport IPython.display as ipd\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nfrom jiwer import wer, cer\nimport math\nimport os\nfrom sklearn.model_selection import train_test_split\nimport random\n\nrandom.seed(75)\nnp.random.seed(75)\ntorch.random.manual_seed(75)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-23T18:24:39.169420Z","iopub.execute_input":"2022-04-23T18:24:39.169751Z","iopub.status.idle":"2022-04-23T18:25:09.422597Z","shell.execute_reply.started":"2022-04-23T18:24:39.169671Z","shell.execute_reply":"2022-04-23T18:25:09.421792Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nTARGET_SR = 16000\nWINDOW_SIZE = 0.02\nWINDOW_STRIDE = 0.01\nHOP_LENGHT = int(TARGET_SR * WINDOW_STRIDE)\nN_FFT = int(TARGET_SR * WINDOW_SIZE)\nWINDOW_LENGHT = N_FFT\n# CHAR2IDX = {char:idx for idx, char in enumerate(['_', ' ', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'])}\nCHAR2IDX = {char:idx for idx, char in enumerate(['_', \" \", \"а\", \"ә\", \"б\", \"в\", \"г\", \"ғ\", \"д\", \"е\",\n                                                 \"ё\", \"ж\", \"з\", \"и\", \"й\", \"к\", \"қ\", \"л\", \"м\", \"н\",\n                                                 \"ң\", \"о\", \"ө\", \"п\", \"р\", \"с\", \"т\", \"у\", \"ұ\", \"ү\",\n                                                 \"ф\", \"х\", \"һ\", \"ц\", \"ч\", \"ш\", \"щ\", \"ъ\", \"ы\", \"і\",\n                                                 \"ь\", \"э\", \"ю\", \"я\"])}\nIDX2CHAR = {idx:char for char, idx in CHAR2IDX.items()}","metadata":{"execution":{"iopub.status.busy":"2022-04-23T18:25:09.424389Z","iopub.execute_input":"2022-04-23T18:25:09.424822Z","iopub.status.idle":"2022-04-23T18:25:09.486144Z","shell.execute_reply.started":"2022-04-23T18:25:09.424782Z","shell.execute_reply":"2022-04-23T18:25:09.485409Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"audio_paths = []\ntext = []\nfor audio_path in tqdm(glob('../input/nu-dataset/ISSAI_KSC_335RS/Audios/*.wav')):\n    audio_paths.append(audio_path)\n    text.append(open('../input/nu-dataset/ISSAI_KSC_335RS/Transcriptions/'+audio_path.split('/')[-1].replace('.wav', '.txt')).read())\n\ndf = pd.DataFrame({'audio_path': audio_paths, 'text': text})\ndf","metadata":{"execution":{"iopub.status.busy":"2022-04-23T18:25:09.489938Z","iopub.execute_input":"2022-04-23T18:25:09.490154Z","iopub.status.idle":"2022-04-23T18:38:01.715401Z","shell.execute_reply.started":"2022-04-23T18:25:09.490125Z","shell.execute_reply":"2022-04-23T18:38:01.714562Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train_df, dev_df = train_test_split(df, test_size=0.1, random_state=75)\ntrain_df = train_df.reset_index(drop=True)\ndev_df = dev_df.reset_index(drop=True)\n\ntrain_df.shape, dev_df.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-23T18:38:01.717670Z","iopub.execute_input":"2022-04-23T18:38:01.717951Z","iopub.status.idle":"2022-04-23T18:38:01.764742Z","shell.execute_reply.started":"2022-04-23T18:38:01.717908Z","shell.execute_reply":"2022-04-23T18:38:01.763958Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class AudioDataset(Dataset):\n    def __init__(self, data_frame, transformation, chr2idx):\n        self.audio_paths = data_frame.audio_path.to_list()\n        self.chr2idx = chr2idx\n        self.labels = data_frame.text.apply(lambda x: self.text_preprocess(x)).to_list()\n#         self.device = device\n        self.transformation = transformation\n        \n        \n    def __len__(self):\n        return len(self.audio_paths)\n    \n    \n    def __getitem__(self, idx):\n        audio_path = self.audio_paths[idx]\n        text = self.labels[idx]\n        signal, sr = torchaudio.load(audio_path)\n        spect = self.audio_preprocess(signal)\n#         transcript = self.text_preprocess(text)\n        \n        return spect, text\n\n    \n    def audio_preprocess(self, signal):\n        if signal.shape[0]==0:\n            signal = signal.squeeze()\n        else:\n            signal = signal.mean(axis=0)\n            \n        spect = self.transformation(signal)\n        spect = torch.log1p(spect)\n        \n        return spect\n    \n    \n    def text_preprocess(self, text):\n        transcript = list(filter(None, [self.chr2idx.get(x) for x in list(text)]))\n        return transcript","metadata":{"execution":{"iopub.status.busy":"2022-04-23T18:38:01.766227Z","iopub.execute_input":"2022-04-23T18:38:01.766492Z","iopub.status.idle":"2022-04-23T18:38:01.777043Z","shell.execute_reply.started":"2022-04-23T18:38:01.766455Z","shell.execute_reply":"2022-04-23T18:38:01.776158Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def batch_preprocessing(batch):\n    batch = sorted(batch, key=lambda sample: sample[0].size(1), reverse=True)\n    longest_sample = batch[0][0]\n    freq_size = longest_sample.size(0)\n    minibatch_size = len(batch)\n    max_seqlength = longest_sample.size(1)\n    inputs = torch.zeros(minibatch_size, 1, freq_size, max_seqlength)\n    input_percentages = torch.FloatTensor(minibatch_size)\n    target_sizes = torch.IntTensor(minibatch_size)\n    targets = []\n    for x in range(minibatch_size):\n        sample = batch[x]\n        tensor = sample[0]\n        target = sample[1]\n        seq_length = tensor.size(1)\n        inputs[x][0].narrow(1, 0, seq_length).copy_(tensor)\n        input_percentages[x] = seq_length / float(max_seqlength)\n        target_sizes[x] = len(target)\n        targets.extend(target)\n    targets = torch.tensor(targets, dtype=torch.long)\n    \n    return inputs, targets, input_percentages, target_sizes\n\n\nclass AudioDataLoader(DataLoader):\n    def __init__(self, *args, **kwargs):\n        super(AudioDataLoader, self).__init__(*args, **kwargs)\n        self.collate_fn = batch_preprocessing","metadata":{"execution":{"iopub.status.busy":"2022-04-23T18:38:01.778664Z","iopub.execute_input":"2022-04-23T18:38:01.779007Z","iopub.status.idle":"2022-04-23T18:38:01.789755Z","shell.execute_reply.started":"2022-04-23T18:38:01.778970Z","shell.execute_reply":"2022-04-23T18:38:01.788766Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"spectogram = torchaudio.transforms.Spectrogram(n_fft=N_FFT,\n                                              win_length=WINDOW_LENGHT,\n                                              hop_length=HOP_LENGHT,\n                                               window_fn=torch.hamming_window\n                                              )","metadata":{"execution":{"iopub.status.busy":"2022-04-23T18:38:01.790938Z","iopub.execute_input":"2022-04-23T18:38:01.791265Z","iopub.status.idle":"2022-04-23T18:38:01.858786Z","shell.execute_reply.started":"2022-04-23T18:38:01.791225Z","shell.execute_reply":"2022-04-23T18:38:01.858081Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train_dataset = AudioDataset(train_df, spectogram, CHAR2IDX)\ndev_dataset = AudioDataset(dev_df, spectogram, CHAR2IDX)","metadata":{"execution":{"iopub.status.busy":"2022-04-23T18:38:01.860111Z","iopub.execute_input":"2022-04-23T18:38:01.860359Z","iopub.status.idle":"2022-04-23T18:38:05.595998Z","shell.execute_reply.started":"2022-04-23T18:38:01.860325Z","shell.execute_reply":"2022-04-23T18:38:05.595264Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"plt.imshow(train_dataset[10000][0].cpu())","metadata":{"execution":{"iopub.status.busy":"2022-04-23T18:38:05.598087Z","iopub.execute_input":"2022-04-23T18:38:05.598348Z","iopub.status.idle":"2022-04-23T18:38:05.893865Z","shell.execute_reply.started":"2022-04-23T18:38:05.598311Z","shell.execute_reply":"2022-04-23T18:38:05.892772Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class DeepSpeech(nn.Module):\n    def __init__(self, n_feature, n_hidden, n_class, dropout = 0, max_clip_relu = 20):\n        super(DeepSpeech, self).__init__()\n        self.n_hidden = n_hidden\n        self.fc_block = nn.Sequential(nn.Linear(n_feature, n_hidden),\n                                     nn.Hardtanh(0, max_clip_relu),\n                                     nn.Dropout(dropout),\n                                     nn.Linear(n_hidden, n_hidden),\n                                     nn.Hardtanh(0, max_clip_relu),\n                                     nn.Dropout(dropout),\n                                     nn.Linear(n_hidden, n_hidden),\n                                     nn.Hardtanh(0, max_clip_relu),\n                                     nn.Dropout(dropout))\n        self.bi_rnn = nn.GRU(n_hidden, n_hidden, bidirectional=True, num_layers=1)\n        self.out = nn.Sequential(nn.Linear(n_hidden, n_hidden),\n                                     nn.Hardtanh(0, max_clip_relu),\n                                     nn.Dropout(dropout),\n                                 nn.Linear(n_hidden, n_class))\n    \n    def forward(self, x, input_sizes):\n        x = x.permute(0, 1, 3, 2)\n        output_sizes = input_sizes\n        x = self.fc_block(x)\n        x = x.squeeze(1)\n        x = x.transpose(0, 1)\n        x, _ = self.bi_rnn(x)\n        x = x[:, :, :self.n_hidden] + x[:, :, self.n_hidden:]\n        x = self.out(x)\n        \n        return x, output_sizes","metadata":{"execution":{"iopub.status.busy":"2022-04-23T18:38:05.897157Z","iopub.execute_input":"2022-04-23T18:38:05.897371Z","iopub.status.idle":"2022-04-23T18:38:05.907260Z","shell.execute_reply.started":"2022-04-23T18:38:05.897345Z","shell.execute_reply":"2022-04-23T18:38:05.906172Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class DeepSpeech2(nn.Module):\n    def __init__(self, n_feature, n_hidden, n_class, dropout = 0, max_clip_relu = 20, n_rnn_layer=3):\n        super(DeepSpeech2, self).__init__()\n        self.n_hidden = n_hidden\n        self.conv_block = nn.Sequential(\n            nn.Conv2d(1, 32, kernel_size=(41, 11), stride=(2, 2), padding=(20, 5)),\n            nn.BatchNorm2d(32),\n            nn.Hardtanh(0, 20, inplace=True),\n            nn.Conv2d(32, 32, kernel_size=(21, 11), stride=(2, 1), padding=(10, 5)),\n            nn.BatchNorm2d(32),\n            nn.Hardtanh(0, 20, inplace=True)\n        )\n        \n        rnn_input_size = int(math.floor((TARGET_SR * WINDOW_SIZE) / 2) + 1)\n        rnn_input_size = int(math.floor(rnn_input_size + 2 * 20 - 41) / 2 + 1)\n        rnn_input_size = int(math.floor(rnn_input_size + 2 * 10 - 21) / 2 + 1)\n        rnn_input_size *= 32\n        self.bi_rnn = nn.GRU(rnn_input_size, n_hidden, bidirectional=True, num_layers=n_rnn_layer)\n        self.out = nn.Sequential(nn.Linear(n_hidden, n_class, bias=False))\n    \n    def forward(self, x, input_sizes):\n        output_sizes = self.get_output_lenght(input_sizes)\n        x = self.conv_block(x)\n        x = x.view(x.size(0), x.size(1) * x.size(2), x.size(3))\n        x = x.permute(2, 0, 1)\n        x, _ = self.bi_rnn(x)\n        x = x[:, :, :self.n_hidden] + x[:, :, self.n_hidden:]\n        t, n, h = x.size(0), x.size(1), x.size(2)\n        x = x.view(t*n, -1)\n        x = self.out(x)\n        x = x.view(t, n, -1)\n        \n        return x, output_sizes\n    \n    def get_output_lenght(self, input_lenght):\n        seq_len = input_lenght\n        for block in self.conv_block.modules():\n            if type(block) == nn.modules.conv.Conv2d:\n                seq_len = ((seq_len + 2 * block.padding[1] - block.dilation[1] * (block.kernel_size[1] - 1) - 1) // block.stride[1] + 1)\n        return seq_len.int()","metadata":{"execution":{"iopub.status.busy":"2022-04-23T18:38:05.908867Z","iopub.execute_input":"2022-04-23T18:38:05.909244Z","iopub.status.idle":"2022-04-23T18:38:05.926651Z","shell.execute_reply.started":"2022-04-23T18:38:05.909201Z","shell.execute_reply":"2022-04-23T18:38:05.925956Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def greedy_decode(probas, idx2char, blank_idx=0):\n    max_values, classes = torch.max(probas, dim=-1)\n    texts = []\n    for sequence in range(len(classes)):\n        sequence_len = len(classes[sequence])\n        text = ''\n        for i in range(sequence_len):\n            char = idx2char[classes[sequence][i].item()]\n            if char != idx2char[blank_idx]:\n                if i != 0 and char == idx2char[classes[sequence][i-1].item()]:\n                    continue\n                text += char\n        texts.append(text)\n        \n    return texts","metadata":{"execution":{"iopub.status.busy":"2022-04-23T18:38:05.927977Z","iopub.execute_input":"2022-04-23T18:38:05.928466Z","iopub.status.idle":"2022-04-23T18:38:05.938908Z","shell.execute_reply.started":"2022-04-23T18:38:05.928426Z","shell.execute_reply":"2022-04-23T18:38:05.938097Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"model = DeepSpeech2(161, 2048, len(IDX2CHAR), 0.2)\nmodel.to(device)\nmodel","metadata":{"execution":{"iopub.status.busy":"2022-04-23T18:38:05.942322Z","iopub.execute_input":"2022-04-23T18:38:05.942979Z","iopub.status.idle":"2022-04-23T18:38:11.327224Z","shell.execute_reply.started":"2022-04-23T18:38:05.942901Z","shell.execute_reply":"2022-04-23T18:38:11.326515Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"train_loader = AudioDataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2)\ndev_loader = AudioDataLoader(dev_dataset, batch_size=4, shuffle=False, num_workers=2) ","metadata":{"execution":{"iopub.status.busy":"2022-04-23T18:38:11.328375Z","iopub.execute_input":"2022-04-23T18:38:11.328635Z","iopub.status.idle":"2022-04-23T18:38:11.333579Z","shell.execute_reply.started":"2022-04-23T18:38:11.328598Z","shell.execute_reply":"2022-04-23T18:38:11.332695Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"loss_fn = nn.CTCLoss()\noptim = torch.optim.Adam(model.parameters())\n\ntrain_history = {'loss' : [],\n                'wer' : []}\n\ndev_history = {'loss' : [],\n                'wer' : [],\n              'cer': []}","metadata":{"execution":{"iopub.status.busy":"2022-04-23T18:38:11.335128Z","iopub.execute_input":"2022-04-23T18:38:11.335639Z","iopub.status.idle":"2022-04-23T18:38:11.356286Z","shell.execute_reply.started":"2022-04-23T18:38:11.335602Z","shell.execute_reply":"2022-04-23T18:38:11.353072Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def get_ground_truth(y, target_sizes):\n    texts = []\n    idx = 0\n    for size in target_sizes:\n        text = ''\n        for i in range(size.item()):\n            text += IDX2CHAR[y[idx+i].item()]\n            \n        texts.append(text)\n        idx += size\n        \n    return texts","metadata":{"execution":{"iopub.status.busy":"2022-04-23T18:38:11.357839Z","iopub.execute_input":"2022-04-23T18:38:11.358283Z","iopub.status.idle":"2022-04-23T18:38:11.369356Z","shell.execute_reply.started":"2022-04-23T18:38:11.358244Z","shell.execute_reply":"2022-04-23T18:38:11.368511Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"for epoch in tqdm(range(20)):\n    total_loss = 0\n    wers = 0\n    model.train()\n    for step, (X, y, input_percentages, target_sizes) in enumerate(tqdm(train_loader)):\n        optim.zero_grad()\n        input_sizes = input_percentages.mul_(int(X.size(3))).int()\n        X, y = X.to(device), y.to(device)\n        preds, output_sizes = model(X, input_sizes)\n        log_probs = nn.functional.log_softmax(preds, dim=-1)\n        loss = loss_fn(log_probs, y, output_sizes, target_sizes)\n        loss.backward()\n        optim.step()\n        total_loss += loss.item()\n        ground_truth = get_ground_truth(y.cpu().detach(), target_sizes)\n        decoded = greedy_decode(nn.functional.softmax(preds.cpu().detach(), dim=-1).transpose(1, 0), IDX2CHAR)\n        wers+= wer(ground_truth, decoded)\n        if step % 1000 == 0:\n            print(f'WER: {round(wers/(step+1), 3)}')\n            print(f'Decoded: {decoded[-1]}')\n            print(f'Ground Truth: {ground_truth[-1]}')\n    \n    train_history['loss'].append(total_loss/len(train_loader))\n    train_history['wer'].append(wers/len(train_loader))\n    \n    model.eval()\n    total_loss = 0\n    wers = 0\n    cers = 0\n    with torch.no_grad():\n        for X, y, input_percentages, target_sizes in tqdm(dev_loader):\n            input_sizes = input_percentages.mul_(int(X.size(3))).int()\n            X, y = X.to(device), y.to(device)\n            preds, output_sizes = model(X, input_sizes)\n            log_probs = nn.functional.log_softmax(preds, dim=-1)\n            probs = nn.functional.softmax(preds, dim=-1).cpu().detach()\n            loss = loss_fn(log_probs, y, output_sizes, target_sizes)\n            total_loss += loss.item()\n            decoded = greedy_decode(probs.transpose(0, 1), IDX2CHAR)\n            ground_truth = get_ground_truth(y.cpu().detach(), target_sizes)\n            wers+= wer(ground_truth, decoded)\n            cers = cer(ground_truth, decoded)\n            \n        dev_history['loss'].append(total_loss/len(dev_loader))\n        dev_history['wer'].append(wers/len(dev_loader))\n        dev_history['cer'].append(cers/len(dev_loader))\n    \n    print(f'Epoch: {epoch+1}')\n    print(f\"Train Loss: {train_history['loss'][-1]}, Train Wer: {train_history['wer'][-1]}\")\n    print(f\"Dev Loss: {dev_history['loss'][-1]}, Dev Wer: {dev_history['wer'][-1]}, Dev Cer: {dev_history['cer'][-1]}\")\n    print('Decoded:', decoded[:3])\n    print('Ground Truth:', ground_truth[:3])\n    torch.save({'model': model.state_dict(),\n               'optimizer': optim.state_dict()},\n               'model_epoch{}_wer{}.pth'.format(epoch, round(dev_history['wer'][-1], 3)))","metadata":{"execution":{"iopub.status.busy":"2022-04-23T18:38:11.370826Z","iopub.execute_input":"2022-04-23T18:38:11.371449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model, 'model.pth')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds.size()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ground_truth","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}